{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "# 1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b6fe4",
   "metadata": {},
   "source": [
    "**Ans:** The main distinction between the two approaches is the use of labeled datasets. To put it simply, supervised learning uses labeled input and output data, while an unsupervised learning algorithm does not. Unsupervised learning models,  in contrast, work on their own to discover the inherent structure of unlabeled data.\n",
    "\n",
    "In Supervised learning, you train the machine using data which is well “labeled.” Unsupervised learning is a machine learning technique, where you do not need to supervise the model. For example, Baby can identify other dogs based on past supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "# 2. Mention a few unsupervised learning applications ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d7be2",
   "metadata": {},
   "source": [
    "**Ans:** The main applications of unsupervised learning include clustering, visualization, dimensionality reduction, finding association rules, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "# 3. What are the three main types of clustering methods? Briefly describe the characteristics of each ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2abf909",
   "metadata": {},
   "source": [
    "**Ans:** The various types of clustering are:\n",
    "\n",
    "The three main types of clustering methods are:\n",
    "\n",
    "Partitioning Clustering:\n",
    "Partitioning clustering aims to divide the data into non-overlapping groups or clusters. The most popular algorithm in this category is k-means. It works by iteratively assigning data points to the nearest cluster centroid and then updating the centroids based on the mean of the assigned points. K-means is efficient and widely used, but it may get stuck in local optima and requires the number of clusters (k) to be specified beforehand.\n",
    "\n",
    "Hierarchical Clustering:\n",
    "Hierarchical clustering creates a tree-like structure of nested clusters, known as a dendrogram. There are two main approaches: agglomerative and divisive. Agglomerative clustering starts with each data point as a separate cluster and then merges the closest clusters iteratively until all data points belong to a single cluster. Divisive clustering, on the other hand, begins with all data points in a single cluster and then divides them recursively based on some criterion. Hierarchical clustering does not require specifying the number of clusters in advance, and it allows for visual representation of clustering at different levels of granularity.\n",
    "\n",
    "Density-Based Clustering:\n",
    "Density-based clustering methods aim to identify regions of high data density and consider points in low-density regions as outliers or noise. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a widely used density-based algorithm. It groups together data points that are close to each other and have a sufficient number of neighbors within a specified radius. It can find clusters of arbitrary shapes and is robust to outliers.\n",
    "\n",
    "Each of these clustering methods has its strengths and weaknesses, and the choice of the appropriate method depends on the characteristics of the data and the specific clustering task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "# 4. Explain how the k-means algorithm determines the consistency of clustering ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fbf5a9",
   "metadata": {},
   "source": [
    "**Ans:** Calculate the Within-Cluster-Sum of Squared Errors (WSS) for different values of k, and choose the k for which WSS  becomes first starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow. Within-Cluster-Sum of  Squared Errors sounds a bit complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "# 5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59735150",
   "metadata": {},
   "source": [
    "**Ans:** K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k -means algorithm, k -medoids chooses datapoints as centers ( medoids or exemplars)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "# 6. What is a dendrogram, and how does it work? Explain how to do it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec88301",
   "metadata": {},
   "source": [
    "**Ans:** A dendrogram is a diagram that shows the attribute distances between each pair of sequentially merged classes After each merging, the distances between all pairs of classes are updated. The distances at which the signatures of classes are merged are used to construct a dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "# 7. What exactly is SSE? What role does it play in the k-means algorithm ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e6d20",
   "metadata": {},
   "source": [
    "**Ans:** I am going to refer to it as SSE, which stands for Sum of Squared Errors. The regression line is the line made using the function we defined above. To get the SSE we calculate the distance for each of the data points from the regression line then square the it, then we add to the sum.\n",
    "\n",
    "The SSE is defined as the sum of the squared Euclidean distances of each point to its closest centroid. Since this is a measure of error, the objective of k-means is to try to minimize this value. The purpose of this figure is to show  that the initialization of the centroids is an important step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "# 8. With a step-by-step algorithm, explain the k-means procedure ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29accb2",
   "metadata": {},
   "source": [
    "**Ans:** k-means is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed apriori. The main idea is to define k centers, one for each cluster.\n",
    "\n",
    "- Step 1: Choose the number of clusters k. \n",
    "- Step 2: Select k random points from the data as centroids.\n",
    "- Step 3: Assign all the points to the closest cluster centroid. \n",
    "- Step 4: Recompute the centroids of newly formed clusters.\n",
    "- Step 5: Repeat steps 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "# 9. In the sense of hierarchical clustering, define the terms single link and complete link ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567b4bd",
   "metadata": {},
   "source": [
    "**Ans:** In single-link (or single linkage) hierarchical clustering, we merge in each step the two clusters whose two closest members have the smallest distance (or: the two clusters with the smallest minimum pairwise distance). Complete-link clustering can also be described using the concept of clique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "# 10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46ef7d",
   "metadata": {},
   "source": [
    "**Ans:** Apriori algorithm assumes that any subset of a frequent itemset must be frequent. Its the algorithm behind Market Basket Analysis. So, according to the principle of Apriori, if {Grapes, Apple, Mango} is frequent, then {Grapes,Mango} must also be frequent. Here is a dataset consisting of six transactions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea362398",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
