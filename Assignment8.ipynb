{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d4f3bd",
   "metadata": {},
   "source": [
    "# 1. What exactly is a feature? Give an example to illustrate your point ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d1cdb",
   "metadata": {},
   "source": [
    "Features are the basic building blocks of datasets. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning. \n",
    "\n",
    "Additionally, different business problems within the same industry do not necessarily require the same features, which is why it is important to have a strong understanding of the business goals of your data science project. Eg age,sex can be features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bc9d9",
   "metadata": {},
   "source": [
    "# 2. What are the various circumstances in which feature construction is required ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0d0e7",
   "metadata": {},
   "source": [
    " The features in your data will directly influence the predictive models you use and the results you can achieve. Your results are dependent on many inter-dependent properties. You need great features that describe the structures inherent in your data. Better features means flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53a7ce",
   "metadata": {},
   "source": [
    "# 3. Describe how nominal variables are encoded ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b1d24",
   "metadata": {},
   "source": [
    "\n",
    "Nominal variables can be encoded using methods such as one-hot encoding, label encoding, or binary encoding. One-hot encoding creates binary vectors for each category, label encoding assigns unique numerical labels to each category, and binary encoding represents categories as binary codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de4555",
   "metadata": {},
   "source": [
    "# 4. Describe how numeric features are converted to categorical features ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ce760",
   "metadata": {},
   "source": [
    "\n",
    "Converting numeric features to categorical features involves transforming continuous numerical values into discrete categories or bins. Here are some common approaches for converting numeric features to categorical features:\n",
    "\n",
    "Binning: Binning involves dividing the range of numeric values into a set of bins or intervals and assigning each value to its corresponding bin. This process groups similar numeric values together and creates discrete categories. The number of bins and their boundaries can be determined based on domain knowledge or statistical techniques such as equal-width binning or equal-frequency binning.\n",
    "\n",
    "Quantile-based Encoding: Quantile-based encoding divides the numeric values into percentile-based bins. Each bin represents a specific range of values, such as quartiles (25%, 50%, 75%), deciles (10%, 20%, ... 90%), or any other desired percentiles. This method ensures that each category has an equal number of observations.\n",
    "\n",
    "Thresholding: Thresholding involves setting specific threshold values to convert a numeric feature into binary categories. Values above the threshold are assigned to one category, and values below the threshold are assigned to another category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6109820",
   "metadata": {},
   "source": [
    "# 5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de65b0",
   "metadata": {},
   "source": [
    " Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of classifiers for obtaining high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51584401",
   "metadata": {},
   "source": [
    "# 6. When is a feature considered irrelevant? What can be said to quantify it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a92af6",
   "metadata": {},
   "source": [
    " Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise. \n",
    "\n",
    "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:\n",
    "\n",
    "- **Wrapper methods** (forward, backward, and stepwise selection)\n",
    "- **Filter methods** (ANOVA, Pearson correlation, variance thresholding)\n",
    "- **Embedded methods** (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21fc3f",
   "metadata": {},
   "source": [
    "# 7. When is a function considered redundant? What criteria are used to identify features that could be redundant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ce42e",
   "metadata": {},
   "source": [
    "**Ans:** If two features `{X1, X2}` are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features. \n",
    "\n",
    "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4a123",
   "metadata": {},
   "source": [
    "# 8. What are the various distance measurements used to determine feature similarity ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81d5c4",
   "metadata": {},
   "source": [
    " Four of the most commonly used distance measures in machine learning are as follows: \n",
    "- Hamming Distance. \n",
    "- Euclidean Distance\n",
    "- Manhattan Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6004e",
   "metadata": {},
   "source": [
    "# 9. State difference between Euclidean and Manhattan distances ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39944ba",
   "metadata": {},
   "source": [
    " Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences. Euclidean distance is extensively applied in analysis of convolutional codes and Trellis codes.\n",
    "\n",
    "Hamming distance is frequently encountered in the analysis of block codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5cfd2",
   "metadata": {},
   "source": [
    "# 10. Distinguish between feature transformation and feature selection ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722dfcff",
   "metadata": {},
   "source": [
    " Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a6f9e",
   "metadata": {},
   "source": [
    "# 11. Make brief notes on any two of the following:\n",
    "\n",
    "1. SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03ce6a",
   "metadata": {},
   "source": [
    "1. SVD (Singular Value Decomposition):\n",
    "\n",
    "- SVD is a matrix factorization technique commonly used in linear algebra and machine learning.\n",
    "- It decomposes a matrix into three separate matrices: U, Σ, and V.\n",
    "- U represents the left singular vectors, Σ is a diagonal matrix containing the singular values, and V represents the right singular vectors.\n",
    "- SVD has various applications, including dimensionality reduction, collaborative filtering, image compression, and natural language processing.\n",
    "- It is particularly useful for reducing the dimensionality of a dataset while preserving important information by retaining the top-k singular values and their corresponding singular vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339b6dc",
   "metadata": {},
   "source": [
    "2. Receiver Operating Characteristic (ROC) Curve:\n",
    "\n",
    "- The ROC curve is a graphical representation of the performance of a binary classification model.\n",
    "- It shows the trade-off between the true positive rate (sensitivity) and the false positive rate (1-specificity) for different classification thresholds.\n",
    "- The curve is created by plotting these rates at various threshold settings, and the area under the curve (AUC) is often used as a summary metric to evaluate the model's performance.\n",
    "- An ideal ROC curve hugs the top-left corner, indicating high sensitivity and low false positive rate, while a random classifier's curve is a diagonal line from the bottom-left to the top-right.\n",
    "- The ROC curve provides insights into the model's discrimination ability and can help in selecting an appropriate threshold based on the desired balance between true positives and false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb98f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
